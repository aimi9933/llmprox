# IDE Python Proxy Server Configuration

# API Settings
API_HOST=0.0.0.0
API_PORT=8000
API_TITLE="IDE Python Proxy Server"
API_VERSION="1.0.0"

# LLM Settings
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=your_openai_api_key_here
DEFAULT_MODEL=codellama

# Context Management
MAX_CONTEXT_LENGTH=8000
CHUNK_OVERLAP_RATIO=0.1
MAX_CHUNKS_PER_REQUEST=10

# Semantic Chunking
EMBEDDING_MODEL=all-MiniLM-L6-v2
SIMILARITY_THRESHOLD=0.7
MAX_CHUNK_SIZE=2000

# Dialog Memory
MAX_DIALOG_HISTORY=20
MEMORY_TTL=3600

# Logging
LOG_LEVEL=INFO